{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "! pip install collections-extended \n",
    "! pip install langdetect\n",
    "! pip install googletrans\n",
    "! pip install seaborn\n",
    "! pip install contractions\n",
    "! pip install yake\n",
    "! pip install rake_nltk\n",
    "! pip install unidecode\n",
    "! pip install googletrans==3.1.0a0\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba434a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect the language\n",
    "def lang_detect(data):\n",
    "  lang = detect(data)\n",
    "  return lang\n",
    "\n",
    "print(df.review_body[0])\n",
    "lang_detect(df.review_body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538599f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate\n",
    "def lang_trans(data):\n",
    "    translor = Translator()\n",
    "    translated_text = translor.translate(data)\n",
    "    return translated_text.text\n",
    "\n",
    "data['translated_reviews'] = data.review_body.apply(lang_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained model\n",
    "path = r\"C:\\Users\\NLP_ipynbs\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing lower case\n",
    "def preprocess_lower(data):\n",
    "    clean_text = [word.lower() for word in word_tokenize(data) if (word not in punctuation) and (word.lower() not in stopword_list) and (word.isalpha())]\n",
    "    return clean_text\n",
    "\n",
    "final_text = data.text.apply(preprocess)\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess remove words whose length is less than 2\n",
    "def filter_1(data):\n",
    "    clean_text = [word for word in data if (len(word)>2)]\n",
    "    return clean_text\n",
    "\n",
    "final_text = final_text.apply(filter_1)\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a674deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert in list\n",
    "final_data = final_text.tolist()\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ae178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "#1. remove spaces, newlines\n",
    "def remove_spaces(data):\n",
    "    clean_text = data.replace('\\\\n',' ').replace('\\\\',' ').replace('\\t',' ')\n",
    "    return clean_text\n",
    "\n",
    "#2. contraction mapping\n",
    "def expand_text(data):\n",
    "    expanded_text = contractions.fix(data)\n",
    "    return expanded_text\n",
    "\n",
    "#3. Handling accented characters\n",
    "def handling_accented(data):\n",
    "    fixed_text = unidecode(data)\n",
    "    return fixed_text\n",
    "\n",
    "#4. Cleaning\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "stopword_list.remove('nor')\n",
    "\n",
    "def clean_data(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    clean_text = [word.lower() for word in tokens if (word not in punctuation) and (word.lower() not in stopword_list) and (len(word)>2) and (word.isalpha())]\n",
    "    return clean_text\n",
    "\n",
    "# Autocorrection\n",
    "def autocorrection(data):\n",
    "    spell = Speller(lang='en')\n",
    "    corrected_text = spell(data)\n",
    "    return corrected_text\n",
    "\n",
    "# lemmatization\n",
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_data = []\n",
    "    for word in data:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        final_data.append(lemmatized_word)\n",
    "    return ' '.join(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "sent = sent_tokenize(text)\n",
    "sent\n",
    "\n",
    "# word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens\n",
    "\n",
    "# Whitespace tokenizer\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1\n",
    "\n",
    "# Normalization\n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text\n",
    "\n",
    "# Remove punctuations or symbols\n",
    "text_without_punct = [word for word in lower_text if word not in punctuation]\n",
    "text_without_punct\n",
    "\n",
    "# Remove stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list\n",
    "\n",
    "text_without_stop = [word for word in text_without_punct if word not in stopword_list]\n",
    "text_without_stop\n",
    "\n",
    "# Contraction mapping\n",
    "text1\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text\n",
    "\n",
    "# stemming and lemmatization\n",
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in text_without_stop:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(f'Original_word >> {word}')\n",
    "    print(f'stemmed_word >> {stemmed_word}')\n",
    "    print(f'lemmatized_word >> {lemmatized_word}')\n",
    "    print('*'*100)\n",
    "    \n",
    "# Handling accented characters\n",
    "accented_character = \"√¢, √™, √Æ, √¥, √ª, √Ç, √ä, √é, √î, √õ,√§, √´, √Ø, √∂, √º, √ø, √Ñ, √ã, √è, √ñ, √ú,√£, √±, √µ, √É, √ë, √ï\"\n",
    "fixed_words = unidecode(accented_character)\n",
    "fixed_words\n",
    "\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "print(spell('mussage'))\n",
    "print(spell('survice'))\n",
    "\n",
    "from textblob import TextBlob\n",
    "a = 'mussage'\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0724b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target column\n",
    "def target_column(data):\n",
    "    if (data==1) or (data==2):\n",
    "        return 0\n",
    "    elif (data==3):\n",
    "        return 1\n",
    "    elif (data==4) or (data==5):\n",
    "        return 2\n",
    "data['target'] = data.stars.apply(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data leakage\n",
    "x_train,x_test,y_train,y_test = train_test_split(data.text,data.label,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_train = x_train.apply(remove_spaces)\n",
    "clean_text_test = x_test.apply(remove_spaces)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(expand_text)\n",
    "clean_text_test = clean_text_test.apply(expand_text)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(handling_accented)\n",
    "clean_text_test = clean_text_test.apply(handling_accented)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(clean_data)\n",
    "clean_text_test = clean_text_test.apply(clean_data)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(lemmatization)\n",
    "clean_text_test = clean_text_test.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c67402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer \n",
    "count = CountVectorizer(max_df=0.95,max_features=1000)\n",
    "count_val_train = count.fit_transform(clean_text_train)\n",
    "count_val_test = count.transform(clean_text_test)\n",
    "\n",
    "count_val_train.A\n",
    "count_val_train.toarray()\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dF\n",
    "pd.DataFrame(count_val_train.A,columns=count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26168c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply Naive bayes algo\n",
    "count_mnb = MultinomialNB()\n",
    "count_mnb.fit(count_val_train.A,y_train)\n",
    "predict_count = count_mnb.predict(count_val_test.A)\n",
    "accuracy_count = accuracy_score(y_test,predict_count)*100\n",
    "accuracy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf091f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "tfidf = TfidfVectorizer(max_df=0.95,max_features=1000)\n",
    "tfidf_train = tfidf.fit_transform(clean_text_train)\n",
    "tfidf_test = tfidf.transform(clean_text_test)\n",
    "\n",
    "tfidf_train\n",
    "tfidf_train.A\n",
    "tfidf_train.toarray()\n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_train.A,columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11629d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply naive bayes on tfidf\n",
    "tfidf_mnb = MultinomialNB()\n",
    "tfidf_mnb.fit(tfidf_train.A,y_train)\n",
    "predict_tfidf = tfidf_mnb.predict(tfidf_test.A)\n",
    "accuracy_tfidf = accuracy_score(y_test,predict_tfidf)*100\n",
    "accuracy_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(count, open('count2.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(mnb_count, open('model2.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f5c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cf4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy\n",
    "# ! python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72520843",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(clean_text)\n",
    "[(token.text,token.pos_) for token in doc1] ## part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity\n",
    "[(token.text,token.label_) for token in doc1.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8784119",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('CARDINAL')\n",
    "spacy.explain('ORG')\n",
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc1,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a33ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1 = nlp('machine')\n",
    "vect1.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1\n",
    "doc2 = nlp(text1)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "    \n",
    "spacy.explain('WORK_OF_ART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_md',disable=['ner'])\n",
    "ruler = nlp2.add_pipe('entity_ruler')\n",
    "patterns = [{'label':'SPORT','pattern':'Basketball'},{'label':'SPORT','pattern':'Lagori'},{'label':'PERSON','pattern':'Rajesh'}]\n",
    "ruler.add_patterns(patterns)\n",
    "text1 = \"Rajesh plays Basketball and Lagori\"\n",
    "doc2 = nlp2(text1)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc1 = nlp(clean_text)\n",
    "entities = []\n",
    "label = []\n",
    "start_position = []\n",
    "end_position = []\n",
    "for ent in doc1.ents:\n",
    "    entities.append(ent.text)\n",
    "    label.append(ent.label_)\n",
    "    start_position.append(ent.start_char)\n",
    "    end_position.append(ent.end_char)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Entities':entities,'label':label,'start_position':start_position,'end_position':end_position})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897132f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b6833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f72604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6508d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install indic_transliteration\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transliteration\n",
    "Syntax : transliterate(text, romanization_style, script)\n",
    "\n",
    "Parameters :\n",
    "test : The text totransliterated\n",
    "romanization_style : The following romanization styles are available :\n",
    "\n",
    "HK = ‚Äòhk‚Äô\n",
    "IAST = ‚Äòiast‚Äô (International Alphabet of Sanskrit Transliteration)\n",
    "ITRANS = ‚Äòitrans‚Äô (Indian languages TRANSliteration)\n",
    "OPTITRANS = ‚Äòoptitrans‚Äô\n",
    "KOLKATA = ‚Äòkolkata‚Äô\n",
    "SLP1 = ‚Äòslp1‚Äô\n",
    "VELTHUIS = ‚Äòvelthuis‚Äô\n",
    "WX = ‚Äòwx‚Äô\n",
    "script : The script to be transliterated into. The following scripts are available :\n",
    "\n",
    "Bengali\n",
    "Devanagari\n",
    "Gujarati\n",
    "Kannada\n",
    "Malayalam\n",
    "Telugu\n",
    "Tamil\n",
    "Oriya\n",
    "Gurmukhi/ Punjabi/ Panjabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language translation\n",
    "from googletrans import Translator\n",
    "def lang_trans(data):\n",
    "    translor = Translator()\n",
    "    translated_text = translor.translate(data)\n",
    "    return translated_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Aap kya kar rahe ho\"\n",
    "print(transliterate(text,sanscript.ITRANS,sanscript.DEVANAGARI))\n",
    "\n",
    "text = \"Aap kese ho\"\n",
    "print(transliterate(text,sanscript.ITRANS,sanscript.DEVANAGARI))\n",
    "\n",
    "text = \"Aap kese ho\"\n",
    "print(transliterate(text,sanscript.OPTITRANS,sanscript.DEVANAGARI))\n",
    "\n",
    "lang_trans('‡§Ü‡§Ö‡§™‡•ç ‡§ï‡•á‡§∏‡•á ‡§π‡•ã')\n",
    "\n",
    "text = \"Tu kashi ahes\"\n",
    "print(transliterate(text,sanscript.ITRANS,sanscript.DEVANAGARI))\n",
    "\n",
    "text = \"Suprabhaata\"\n",
    "print(transliterate(text,sanscript.IAST,sanscript.DEVANAGARI))\n",
    "\n",
    "text = \"Suprabhaata\"\n",
    "print(transliterate(text,sanscript.IAST,sanscript.TELUGU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis\n",
    "import emoji\n",
    "emoji.demojize('üî•extinguisher')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
